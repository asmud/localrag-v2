# LocalRAG Environment Configuration
# Copy this file to .env and update the values

# Application Settings
APP_NAME=LocalRAG
APP_VERSION=0.1.0
DEBUG=false
LOG_LEVEL=INFO
HOST=0.0.0.0
PORT=8000

# Database Configuration
# PostgreSQL
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=localrag
POSTGRES_USER=localrag
POSTGRES_PASSWORD=localrag_password
DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

# Neo4j Graph Database
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j_password
NEO4J_HTTP_PORT=7474
NEO4J_BOLT_PORT=7687
# Neo4j Docker-specific configuration
NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}
NEO4J_dbms_default__database=neo4j
NEO4J_dbms_memory_heap_initial__size=512m
NEO4J_dbms_memory_heap_max__size=2G
NEO4J_dbms_memory_pagecache_size=1G
NEO4J_dbms_security_procedures_unrestricted=apoc.*
NEO4J_dbms_security_procedures_allowlist=apoc.*

# Redis Cache
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_URL=redis://redis:6379/0
REDIS_MAXMEMORY=512mb

# Model Configuration
MODELS_PATH=./models
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EXTRACTION_MODEL=asmud/cahya-indonesian-ner-tuned
AUTO_DOWNLOAD_MODELS=true
HF_TOKEN=

# Data Paths
DATA_PATH=./data
UPLOADS_PATH=./data/uploads
PROCESSED_PATH=./data/processed

# RAG Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_CHUNK_CHARACTERS=8000
ENABLE_CHUNK_SPLITTING=true
MAX_TOKENS=2048
TEMPERATURE=0.7
TOP_K=10
SIMILARITY_THRESHOLD=0.7

# Knowledge Graph
KG_UPDATE_INTERVAL=300
KG_BATCH_SIZE=100

# API Configuration
API_V1_PREFIX=/api/v1
CORS_ORIGINS=http://localhost:3000,http://localhost:8080
MAX_UPLOAD_SIZE=100MB
RATE_LIMIT_PER_MINUTE=60

# Monitoring and Logging
ENABLE_METRICS=true
METRICS_PORT=9090
LOG_FILE_PATH=./logs/localrag.log
LOG_ROTATION="1 day"
LOG_RETENTION="30 days"

# OCR Configuration
OCR_ENGINE=markitdown
OCR_FALLBACK_ENGINE=tesseract
OCR_LANGUAGES=en,id
OCR_CONFIDENCE_THRESHOLD=0.6
OCR_ENABLE_PREPROCESSING=true
OCR_ENABLE_POSTPROCESSING=true
OCR_GPU_ENABLED=true
OCR_DPI_THRESHOLD=300
OCR_MAX_IMAGE_SIZE=4096

# OCR Model Pre-downloading Configuration
OCR_PREDOWNLOAD_MODELS=true
OCR_MODELS_CACHE_DIR=./models/ocr
OCR_DOWNLOAD_TIMEOUT=600
OCR_LAZY_LOADING=true

# OCR Engine Control Configuration
# Disable problematic engines (comma-separated list, e.g., "paddleocr" if causing segfaults)
OCR_DISABLE_ENGINES=
OCR_FORCE_CPU_MODE=false
OCR_SAFE_MODE=true
OCR_INITIALIZATION_TIMEOUT=60

# Development Settings
RELOAD=false
WORKERS=1

# LLM Configuration for Chat Response Generation
# Set ENABLE_LLM_CHAT=true to use LLM for chat responses instead of extractive summarization
ENABLE_LLM_CHAT=false
LLM_PROVIDER=openai
LLM_MODEL=gpt-3.5-turbo
LLM_API_KEY=
LLM_ENDPOINT=
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1024
LLM_STREAMING=true
LLM_TIMEOUT=30
LLM_FALLBACK_TO_EXTRACTIVE=true

# Example configurations for different LLM providers:

# OpenAI Configuration
# ENABLE_LLM_CHAT=true
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-3.5-turbo
# LLM_API_KEY=sk-your-openai-api-key-here

# OpenAI with GPT-4
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4
# LLM_API_KEY=sk-your-openai-api-key-here

# Anthropic Claude Configuration  
# ENABLE_LLM_CHAT=true
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-3-sonnet-20240229
# LLM_API_KEY=sk-ant-your-anthropic-api-key-here

# Local Ollama Configuration (requires Ollama server running)
# ENABLE_LLM_CHAT=true
# LLM_PROVIDER=ollama
# LLM_MODEL=llama2
# LLM_ENDPOINT=http://localhost:11434